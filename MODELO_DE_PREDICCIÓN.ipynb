{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maynex69/Gestion-de-ingresos-hospitalarios/blob/main/MODELO_DE_PREDICCI%C3%93N.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ekFtXXOuiRhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57003344-86c9-4266-d5d7-94046ba2cd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/df.csv')\n",
        "!pip install --upgrade tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0EROzyG-iwvg"
      },
      "outputs": [],
      "source": [
        "# Selecciona solo las columnas que deseas conservar\n",
        "df = df[['Age', 'Medication', 'Medical Condition']]\n",
        "\n",
        "# Eliminar columnas no deseadas\n",
        "df = df.drop(columns=[col for col in df.columns if col not in ['Age', 'Medication', 'Medical Condition']])\n",
        "\n",
        "# Obtener los 5 medicamentos más comunes\n",
        "top_5_medications = df['Medication'].value_counts().index[:5]\n",
        "\n",
        "# Filtrar el DataFrame para incluir solo esos medicamentos\n",
        "df_reduced = df[df['Medication'].isin(top_5_medications)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMnmgxlj5Ik"
      },
      "source": [
        "# Desarrollo del modelo de ML\n",
        "\n",
        "```\n",
        "# Esto tiene formato de código\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "# Preparar los datos\n",
        "# Separar características y etiqueta\n",
        "X = df_reduced[['Age', 'Medical Condition']]\n",
        "y = df_reduced['Medication']\n",
        "\n",
        "# Codificar la columna 'Medication' a números enteros\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Convertir las etiquetas a formato categórico (one-hot encoding) solo para la red neuronal\n",
        "y_encoded_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Separar características numéricas de las categóricas\n",
        "X_numeric = X[['Age']].values\n",
        "X_categorical = X[['Medical Condition']].values\n",
        "\n",
        "# Escalar datos numéricos\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# OneHotEncoding para la columna 'Medical Condition'\n",
        "encoder = OneHotEncoder()\n",
        "X_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n",
        "\n",
        "# Concatenar columnas escaladas y codificadas\n",
        "X_prepared = np.concatenate([X_numeric_scaled, X_categorical_encoded], axis=1)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_prepared, y_encoded, test_size=0.4, random_state=42)\n",
        "\n",
        "# Crear el modelo de red neuronal\n",
        "def create_nn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Wrapper para el modelo de Keras\n",
        "class KerasModelWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, epochs=50, batch_size=32):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)  # Asegura que classes_ esté definido\n",
        "        self.model = create_nn_model(X.shape[1])\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        return np.argmax(predictions, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Definir el modelo de Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Ajustar el modelo RandomForest\n",
        "grid_search.fit(X_train, y_train)  # Ajustar el modelo RandomForest con etiquetas originales\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Crear el modelo de la red neuronal y envolver con KerasModelWrapper\n",
        "nn_model = KerasModelWrapper(epochs=50, batch_size=32)\n",
        "\n",
        "# Crear el modelo de Stacking\n",
        "estimators = [\n",
        "    ('rf', best_rf),\n",
        "    ('nn', nn_model)\n",
        "]\n",
        "\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Ajustar el modelo de Stacking\n",
        "stacking_clf.fit(X_train, y_train)  # Usar etiquetas originales para el StackingClassifier\n",
        "\n",
        "# Evaluar el modelo de Stacking\n",
        "stacking_accuracy = stacking_clf.score(X_test, y_test)  # Usar etiquetas originales para evaluación\n",
        "print(\"Precisión en el conjunto de prueba combinando Random Forest y Red Neuronal con Stacking:\", stacking_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq931fEKO0qs",
        "outputId": "7ec2d924-3ed7-4d60-ba53-7d2ded8e1f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Precisión en el conjunto de prueba combinando Random Forest y Red Neuronal con Stacking: 0.7573196857890978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "# Título de la aplicación\n",
        "st.title(\"Predicción de Medicamentos Basada en Condición Médica y Edad\")\n",
        "\n",
        "# Cargar archivo CSV\n",
        "uploaded_file = st.file_uploader(\"Sube tu archivo CSV\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # Mostrar primeras filas del DataFrame\n",
        "    st.write(\"Vista previa de los datos:\")\n",
        "    st.write(df.head())\n",
        "\n",
        "    # Preparar los datos\n",
        "    X = df[['Age', 'Medical Condition']]\n",
        "    y = df['Medication']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    y_encoded_cat = to_categorical(y_encoded)\n",
        "\n",
        "    X_numeric = X[['Age']].values\n",
        "    X_categorical = X[['Medical Condition']].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "    encoder = OneHotEncoder()\n",
        "    X_categorical_encoded = encoder.fit_transform(X_categorical).toarray()\n",
        "\n",
        "    X_prepared = np.concatenate([X_numeric_scaled, X_categorical_encoded], axis=1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_prepared, y_encoded, test_size=0.4, random_state=42)\n",
        "\n",
        "    def create_nn_model(input_dim):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    class KerasModelWrapper(BaseEstimator, ClassifierMixin):\n",
        "        def __init__(self, epochs=50, batch_size=32):\n",
        "            self.epochs = epochs\n",
        "            self.batch_size = batch_size\n",
        "            self.model = None\n",
        "            self.classes_ = None\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            self.classes_ = np.unique(y)\n",
        "            self.model = create_nn_model(X.shape[1])\n",
        "            self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
        "            return self\n",
        "\n",
        "        def predict(self, X):\n",
        "            predictions = self.model.predict(X)\n",
        "            return np.argmax(predictions, axis=1)\n",
        "\n",
        "        def predict_proba(self, X):\n",
        "            return self.model.predict(X)\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_rf = grid_search.best_estimator_\n",
        "\n",
        "    nn_model = KerasModelWrapper(epochs=50, batch_size=32)\n",
        "\n",
        "    estimators = [\n",
        "        ('rf', best_rf),\n",
        "        ('nn', nn_model)\n",
        "    ]\n",
        "\n",
        "    stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "    stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "    stacking_accuracy = stacking_clf.score(X_test, y_test)\n",
        "\n",
        "    st.write(\"Precisión del modelo de Stacking en el conjunto de prueba:\", stacking_accuracy)\n"
      ],
      "metadata": {
        "id": "WGzI6Z_Ewfi9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBdIS4/5rCl++d4weqNYXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}